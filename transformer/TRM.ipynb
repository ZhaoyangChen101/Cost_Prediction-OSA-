{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from func.transformer import TransformerDataset, TransformerModel, MyLoss\n",
    "from func.execution import eval_epoch, fit\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import pickle\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on Mac M1! Device was set as \"mps\"\n"
     ]
    }
   ],
   "source": [
    "if(torch.has_mps):\n",
    "    device = torch.device('mps')\n",
    "    print('Training on Mac M1! Device was set as \"mps\"')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print('Training on CPU! Device was set as \"cpu\"') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'batch_size': 64,\n",
    "          'lr': 0.0001,\n",
    "          'func':'log10',\n",
    "          'stat_path': 'stat_test/',\n",
    "          'version': 'draft',\n",
    "          'train_percent': 0.7,\n",
    "          'val_percent':0.15,\n",
    "          'epoch': 50,\n",
    "          'max_len_i': 187,\n",
    "          'max_pos': 187,\n",
    "          'emb_size': 128,\n",
    "          'num_heads': 8,\n",
    "          'num_encoder_layers': 2,\n",
    "          'num_decoder_layers': 2,\n",
    "          'dropout_p': 0.1,\n",
    "         }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine = torch.Tensor([])\n",
    "id2cost = torch.Tensor([])\n",
    "cost_tensor = torch.Tensor([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_visit=187, which is the default number\n",
    "data = TransformerDataset(combine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Splitting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of patients for training is: 3421\n",
      "Number of patients for validation is: 733\n",
      "Number of patients for testing is: 733\n"
     ]
    }
   ],
   "source": [
    "num_patients = len(data)\n",
    "\n",
    "# divide data into training/validation/testing sets\n",
    "train_percent = params['train_percent']\n",
    "val_percent = params['val_percent']\n",
    "\n",
    "num_train = int(np.around(train_percent * num_patients))\n",
    "num_val = int(np.around(val_percent * num_patients))\n",
    "num_test = num_patients - num_train - num_val\n",
    "print(f\"Number of patients for training is: {num_train}\")\n",
    "print(f\"Number of patients for validation is: {num_val}\")\n",
    "print(f\"Number of patients for testing is: {num_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length for training dataset is: 3421\n",
      "Length for validation dataset is: 733\n",
      "Length for testing dataset is: 733\n"
     ]
    }
   ],
   "source": [
    "train, val, test = torch.utils.data.random_split(data, [num_train, num_val, num_test])\n",
    "print(f\"Length for training dataset is: {len(train)}\")\n",
    "print(f\"Length for validation dataset is: {len(val)}\")\n",
    "print(f\"Length for testing dataset is: {len(test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Batchify DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = params['batch_size']\n",
    "train_DataLoader = DataLoader(dataset=train, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "val_DataLoader = DataLoader(dataset=val, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "test_DataLoader = DataLoader(dataset=test, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter setting\n",
    "max_len_i = params['max_len_i']\n",
    "\n",
    "cost_vocab_size = len(id2cost_type)  # 53\n",
    "age_vocab_size = dict_vocab_size['age']  # 93\n",
    "gender_vocab_size = dict_vocab_size['gender']  # 2\n",
    "diff_vocab_size = dict_vocab_size['diff']  # 5714\n",
    "department_vocab_size = dict_vocab_size['department']  # 15\n",
    "specialist_vocab_size = dict_vocab_size['specialist']  # 34\n",
    "visit_type_vocab_size = dict_vocab_size['visit_type']  # 8\n",
    "\n",
    "max_pos = params['max_pos']\n",
    "emb_size = params['emb_size']\n",
    "num_heads = params['num_heads']\n",
    "num_encoder_layers = params['num_encoder_layers'] \n",
    "num_decoder_layers = params['num_decoder_layers']\n",
    "dropout_p = params['dropout_p']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model initiation\n",
    "model = TransformerModel(cost_vocab_size=cost_vocab_size,\n",
    "                          age_vocab_size=age_vocab_size,\n",
    "                          gender_vocab_size=gender_vocab_size,\n",
    "                          diff_vocab_size=diff_vocab_size,\n",
    "                          department_vocab_size=department_vocab_size,\n",
    "                          specialist_vocab_size=specialist_vocab_size,\n",
    "                          visit_type_vocab_size=visit_type_vocab_size,\n",
    "                          max_pos=max_pos,\n",
    "                          emb_size=emb_size,\n",
    "                          num_heads=num_heads,\n",
    "                          num_encoder_layers=num_encoder_layers,\n",
    "                          num_decoder_layers=num_decoder_layers,\n",
    "                          dropout_p=dropout_p,\n",
    "                         ).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and validating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = MyLoss()\n",
    "lr = params['lr']\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
    "epochs = params['epoch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Epoch 1 ---------------------------------------------\n",
      "train batch: 1/53, train loss: 12.347\n",
      "top3 acc: 2.01%, top5 acc: 4.31%, top10 acc: 9.48%\n",
      "MAE: 13352.039459228516\n",
      "MSE: 179154795.86754605\n",
      "RMSE: 13384.871903292391\n",
      "R-Squared: -280.73761408104735\n",
      "eval batch: 1/11, eval loss: 11.093\n",
      "top3 acc: 2.66%, top5 acc: 3.19%, top10 acc: 6.38%\n",
      "MAE: 12508.011657714844\n",
      "MSE: 156851045.2101695\n",
      "RMSE: 12524.018732426484\n",
      "R-Squared: -1557.175367049934\n",
      "Epoch 1 summary:        \n",
      "\ttrain -> avg loss: 12.347        \n",
      "\t         MAE:13352.039, MSE:179154795.868, RMSE:13384.872, R2: -280.738         \n",
      "\t         top3 acc: 2.01%, top5 acc: 4.31%, top10 acc: 9.48%        \n",
      "\tval   -> avg loss: 11.093        \n",
      "\t         MAE:12508.012, MSE:156851045.210, RMSE:12524.019, R2: -1557.175         \n",
      "\t         top3 acc: 2.66%, top5 acc: 3.19%, top10 acc: 6.38%        \n",
      "\ttime  -> 5.752218961715698s\n",
      "The best_model with least val loss is in epoch 1:\n",
      "avg total loss: 11.092607498168945\n",
      "MAE:12508.011657714844, MSE:156851045.2101695, RMSE:12524.018732426484, R-Squared:-1557.175367049934\n",
      "top3:2.6595744680851063, top5:3.1914893617021276, top10:6.382978723404255\n",
      "TransformerModel(\n",
      "  (cost_embedding): Embedding(53, 128)\n",
      "  (age_embedding): Embedding(93, 128)\n",
      "  (gender_embedding): Embedding(2, 128)\n",
      "  (diff_embedding): Embedding(5714, 128)\n",
      "  (department_embedding): Embedding(15, 128)\n",
      "  (specialist_embedding): Embedding(34, 128)\n",
      "  (visit_type_embedding): Embedding(8, 128)\n",
      "  (pos_embedding): Embedding(187, 128)\n",
      "  (tgt_embedding): Embedding(53, 128)\n",
      "  (transformer): Transformer(\n",
      "    (encoder): TransformerEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0): TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
      "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
      "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (decoder): TransformerDecoder(\n",
      "      (layers): ModuleList(\n",
      "        (0): TransformerDecoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (multihead_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
      "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): TransformerDecoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (multihead_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
      "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (out): Linear(in_features=128, out_features=53, bias=True)\n",
      ")\n",
      "total time: 00:00:05\n"
     ]
    }
   ],
   "source": [
    "train_summary, val_summary, best_model = fit(train_DataLoader, val_DataLoader, model, optimizer, loss_function, id2cost, cost_tensor, params, device, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the best model\n",
    "PATH_model = params['stat_path'] + 'model_' + params['func'] + '_' + params['version']\n",
    "torch.save(best_model.state_dict(), PATH_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the best model's state_dict\n",
    "loaded_model = TransformerModel(cost_vocab_size=cost_vocab_size,\n",
    "                                 age_vocab_size=age_vocab_size,\n",
    "                                 gender_vocab_size=gender_vocab_size,\n",
    "                                 diff_vocab_size=diff_vocab_size,\n",
    "                                 department_vocab_size=department_vocab_size,\n",
    "                                 specialist_vocab_size=specialist_vocab_size,\n",
    "                                 visit_type_vocab_size=visit_type_vocab_size,\n",
    "                                 max_pos=max_pos,\n",
    "                                 emb_size=emb_size,\n",
    "                                 num_heads=num_heads,\n",
    "                                 num_encoder_layers=num_encoder_layers,\n",
    "                                 num_decoder_layers=num_decoder_layers,\n",
    "                                 dropout_p=dropout_p,\n",
    "                                ).to(device)\n",
    "loaded_model.load_state_dict(torch.load(PATH_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval batch: 1/11, eval loss: 11.074\n",
      "top3 acc: 1.45%, top5 acc: 2.02%, top10 acc: 5.78%\n",
      "MAE: 12439.210876464844\n",
      "MSE: 155442007.55766794\n",
      "RMSE: 12467.638411410075\n",
      "R-Squared: -642.0649244024628\n"
     ]
    }
   ],
   "source": [
    "test_results = eval_epoch(test_DataLoader, model, loss_function, id2cost, cost_tensor, params, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test summary:        \n",
      "\tavg loss: 11.074        \n",
      "\tMAE:12439.211, MSE:155442007.558, RMSE:12467.638, R2: -642.065         \n",
      "\ttop3 acc: 1.45%, top5 acc: 2.02%, top10 acc: 5.78%\n"
     ]
    }
   ],
   "source": [
    "# print results\n",
    "epoch_loss_test = test_results[0]\n",
    "epoch_top3_test = test_results[1]\n",
    "epoch_top5_test = test_results[2]\n",
    "epoch_top10_test = test_results[3]\n",
    "epoch_mae_test = test_results[4]\n",
    "epoch_mse_test = test_results[5]\n",
    "epoch_rmse_test = test_results[6]\n",
    "epoch_r2_test = test_results[7]\n",
    "print(f\"Test summary:\\\n",
    "        \\n\\tavg loss: {epoch_loss_test:.3f}\\\n",
    "        \\n\\tMAE:{epoch_mae_test:.3f}, MSE:{epoch_mse_test:.3f}, RMSE:{epoch_rmse_test:.3f}, R2: {epoch_r2_test:.3f} \\\n",
    "        \\n\\ttop3 acc: {epoch_top3_test:.2f}%, top5 acc: {epoch_top5_test:.2f}%, top10 acc: {epoch_top10_test:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
