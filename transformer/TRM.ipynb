{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from func.transformer import TransformerDataset, TransformerModel, MyLoss\n",
    "from func.execution import eval_epoch, fit\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import pickle\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(torch.has_mps):\n",
    "    device = torch.device('mps')\n",
    "    print('Training on Mac M1! Device was set as \"mps\"')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print('Training on CPU! Device was set as \"cpu\"') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'batch_size': 64,\n",
    "          'lr': 0.0001,\n",
    "          'func':'log10',\n",
    "          'stat_path': 'stat_test/',\n",
    "          'version': 'draft',\n",
    "          'train_percent': 0.7,\n",
    "          'val_percent':0.15,\n",
    "          'epoch': 50,\n",
    "          'max_len_i': 187,\n",
    "          'max_pos': 187,\n",
    "          'emb_size': 128,\n",
    "          'num_heads': 8,\n",
    "          'num_encoder_layers': 2,\n",
    "          'num_decoder_layers': 2,\n",
    "          'dropout_p': 0.1,\n",
    "         }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine = torch.Tensor([])\n",
    "id2cost = torch.Tensor([])\n",
    "cost_tensor = torch.Tensor([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_visit=187, which is the default number\n",
    "data = TransformerDataset(combine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Splitting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_patients = len(data)\n",
    "\n",
    "# divide data into training/validation/testing sets\n",
    "train_percent = params['train_percent']\n",
    "val_percent = params['val_percent']\n",
    "\n",
    "num_train = int(np.around(train_percent * num_patients))\n",
    "num_val = int(np.around(val_percent * num_patients))\n",
    "num_test = num_patients - num_train - num_val\n",
    "print(f\"Number of patients for training is: {num_train}\")\n",
    "print(f\"Number of patients for validation is: {num_val}\")\n",
    "print(f\"Number of patients for testing is: {num_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test = torch.utils.data.random_split(data, [num_train, num_val, num_test])\n",
    "print(f\"Length for training dataset is: {len(train)}\")\n",
    "print(f\"Length for validation dataset is: {len(val)}\")\n",
    "print(f\"Length for testing dataset is: {len(test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Batchify DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = params['batch_size']\n",
    "train_DataLoader = DataLoader(dataset=train, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "val_DataLoader = DataLoader(dataset=val, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "test_DataLoader = DataLoader(dataset=test, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter setting\n",
    "max_len_i = params['max_len_i']\n",
    "\n",
    "cost_vocab_size = len(id2cost_type)  # 53\n",
    "age_vocab_size = dict_vocab_size['age']  # 93\n",
    "gender_vocab_size = dict_vocab_size['gender']  # 2\n",
    "diff_vocab_size = dict_vocab_size['diff']  # 5714\n",
    "department_vocab_size = dict_vocab_size['department']  # 15\n",
    "specialist_vocab_size = dict_vocab_size['specialist']  # 34\n",
    "visit_type_vocab_size = dict_vocab_size['visit_type']  # 8\n",
    "\n",
    "max_pos = params['max_pos']\n",
    "emb_size = params['emb_size']\n",
    "num_heads = params['num_heads']\n",
    "num_encoder_layers = params['num_encoder_layers'] \n",
    "num_decoder_layers = params['num_decoder_layers']\n",
    "dropout_p = params['dropout_p']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model initiation\n",
    "model = TransformerModel(cost_vocab_size=cost_vocab_size,\n",
    "                          age_vocab_size=age_vocab_size,\n",
    "                          gender_vocab_size=gender_vocab_size,\n",
    "                          diff_vocab_size=diff_vocab_size,\n",
    "                          department_vocab_size=department_vocab_size,\n",
    "                          specialist_vocab_size=specialist_vocab_size,\n",
    "                          visit_type_vocab_size=visit_type_vocab_size,\n",
    "                          max_pos=max_pos,\n",
    "                          emb_size=emb_size,\n",
    "                          num_heads=num_heads,\n",
    "                          num_encoder_layers=num_encoder_layers,\n",
    "                          num_decoder_layers=num_decoder_layers,\n",
    "                          dropout_p=dropout_p,\n",
    "                         ).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and validating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = MyLoss()\n",
    "lr = params['lr']\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
    "epochs = params['epoch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_summary, val_summary, best_model = fit(train_DataLoader, val_DataLoader, model, optimizer, loss_function, id2cost, cost_tensor, params, device, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the best model\n",
    "PATH_model = params['stat_path'] + 'model_' + params['func'] + '_' + params['version']\n",
    "torch.save(best_model.state_dict(), PATH_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model's state_dict\n",
    "loaded_model = TransformerModel(cost_vocab_size=cost_vocab_size,\n",
    "                                 age_vocab_size=age_vocab_size,\n",
    "                                 gender_vocab_size=gender_vocab_size,\n",
    "                                 diff_vocab_size=diff_vocab_size,\n",
    "                                 department_vocab_size=department_vocab_size,\n",
    "                                 specialist_vocab_size=specialist_vocab_size,\n",
    "                                 visit_type_vocab_size=visit_type_vocab_size,\n",
    "                                 max_pos=max_pos,\n",
    "                                 emb_size=emb_size,\n",
    "                                 num_heads=num_heads,\n",
    "                                 num_encoder_layers=num_encoder_layers,\n",
    "                                 num_decoder_layers=num_decoder_layers,\n",
    "                                 dropout_p=dropout_p,\n",
    "                                ).to(device)\n",
    "loaded_model.load_state_dict(torch.load(PATH_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = eval_epoch(test_DataLoader, model, loss_function, id2cost, cost_tensor, params, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print results\n",
    "epoch_loss_test = test_results[0]\n",
    "epoch_top3_test = test_results[1]\n",
    "epoch_top5_test = test_results[2]\n",
    "epoch_top10_test = test_results[3]\n",
    "epoch_mae_test = test_results[4]\n",
    "epoch_mse_test = test_results[5]\n",
    "epoch_rmse_test = test_results[6]\n",
    "epoch_r2_test = test_results[7]\n",
    "print(f\"Test summary:\\\n",
    "        \\n\\tavg loss: {epoch_loss_test:.3f}\\\n",
    "        \\n\\tMAE:{epoch_mae_test:.3f}, MSE:{epoch_mse_test:.3f}, RMSE:{epoch_rmse_test:.3f}, R2: {epoch_r2_test:.3f} \\\n",
    "        \\n\\ttop3 acc: {epoch_top3_test:.2f}%, top5 acc: {epoch_top5_test:.2f}%, top10 acc: {epoch_top10_test:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
